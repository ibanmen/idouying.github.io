---
title: Kubernetes 部署实战从0-1
date: 2020-06-27 15:02:00
categories: Kubernetes
tags:
  - Kubernetes
---

Kubernetes 部署实战

<!--more-->

## 配置要求

高可用集群安装具备如下特点：

- CentOS 7.8
- Kubernetes v1.18.x
  - calico 3.13.1
  - nginx-ingress 1.5.5
- Docker 19.03.8
- 三个 master 组成主节点集群，通过内网 loader balancer 实现负载均衡；至少需要三个 master 节点才可组成高可用集群，否则会出现 脑裂 现象
- 多个 worker 组成工作节点集群，通过外网 loader balancer 实现负载均衡

### 检查 centos / hostname

```bash
# 在 master 节点和 worker 节点都要执行
cat /etc/redhat-release

# 此处 hostname 的输出将会是该机器在 Kubernetes 集群中的节点名字
# 不能使用 localhost 作为节点的名字
hostname

# 请使用 lscpu 命令，核对 CPU 信息
# Architecture: x86_64    本安装文档不支持 arm 架构
# CPU(s):       2         CPU 内核数量不能低于 2
lscpu
```

### 检查网络

> 在所有节点执行命令

``` {2,11,13}
[root@demo-master-a-1 ~]$ ip route show
default via 172.21.0.1 dev eth0
169.254.0.0/16 dev eth0 scope link metric 1002
172.21.0.0/20 dev eth0 proto kernel scope link src 172.21.0.12

[root@demo-master-a-1 ~]$ ip address
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:16:3e:12:a4:1b brd ff:ff:ff:ff:ff:ff
    inet 172.17.216.80/20 brd 172.17.223.255 scope global dynamic eth0
       valid_lft 305741654sec preferred_lft 305741654sec
```

kubelet使用的IP地址：

- `ip route show` 命令中，可以知道机器的默认网卡，通常是 `eth0`，如 `default via 172.21.0.1 dev eth0`
- `ip address` 命令中，可显示默认网卡的 IP 地址，Kubernetes 将使用此 IP 地址与集群内的其他节点通信，如 `172.17.216.80`
- 所有节点上 Kubernetes 所使用的 IP 地址必须可以互通（无需 NAT 映射、无安全组或防火墙隔离）

**注意事项：**

- 任意节点 centos 版本为 7.8
- 任意节点 CPU 内核数量大于等于 2，且内存大于等于 4G
- 任意节点 hostname 不是 localhost，且不包含下划线、小数点、大写字母
- 任意节点都有固定的内网 IP 地址
- 任意节点都只有一个网卡，如果有特殊目的，可以在完成 K8S 安装后再增加新的网卡
- 任意节点上 Kubelet使用的 IP 地址可互通（无需 NAT 映射即可相互访问），且没有防火墙、安全组隔离
- 任意节点不会直接使用 docker run 或 docker-compose 运行容器

### 修改 hostname

```bash
# 修改 hostname
hostnamectl set-hostname your-new-host-name
# 查看修改结果
hostnamectl status
# 设置 hostname 解析
echo "127.0.0.1   $(hostname)" >> /etc/hosts
```

## 集群部署

### 安装docker及kubelet

**使用 root 身份在所有节点执行如下代码，以安装软件：**

- docker
- nfs-utils
- kubectl / kubeadm / kubelet

```bash
# 在 master 节点和 worker 节点都要执行
# 最后一个参数 1.18.4 用于指定 kubenetes 版本，支持所有 1.18.x 版本的安装
# docker hub 镜像请根据自己网络的情况任选一个
# 腾讯云 docker hub 镜像
# export REGISTRY_MIRROR="https://mirror.ccs.tencentyun.com"
# DaoCloud 镜像
# export REGISTRY_MIRROR="http://f1361db2.m.daocloud.io"
# 华为云镜像
# export REGISTRY_MIRROR="https://05f073ad3c0010ea0f4bc00b7105ec20.mirror.swr.myhuaweicloud.com"
# 阿里云 docker hub 镜像
export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com
curl -sSL https://xinlichao.cn/resource/back-end/k8s/k8s-0-1/install_kubelet.sh | sh -s 1.18.4
```

### 初始化API Server

#### 创建 ApiServer 的 Load Balancer（私网）

- 监听端口：6443 / TCP
- 后端资源组：包含 demo-master-a-1, demo-master-a-2, demo-master-a-3
- 后端端口：6443
- 开启 按源地址保持会话

假设完成创建以后，Load Balancer 的 ip 地址为 x.x.x.x
> 内部负载是为了给 master 中 ApiServer、ETCD使用，如果其中一台 master 节点宕机，可以实现自动漂移
> 根据每个人实际的情况不同，实现 LoadBalancer 的方式不一样，本文不详细阐述如何搭建 LoadBalancer，请读者自行解决，可以考虑的选择有：
> * nginx
> * haproxy
> * keepalived
> * 云供应商提供的负载均衡产品（建议使用aliyun内网负载均衡）

### 初始化第一个master节点

> 以 root 身份在 demo-master-a-1 机器上执行
> 初始化 master 节点时，如果因为中间某些步骤的配置出错，想要重新初始化 master 节点，请先执行 kubeadm reset 操作

**关于初始化时用到的环境变量：**

- APISERVER_NAME 不能是 master 的 hostname
- APISERVER_NAME 必须全为小写字母、数字、小数点，不能包含减号
- POD_SUBNET pod 所使用的网段不能与 master节点/worker节点 所在的网段重叠。该字段的取值为一个 CIDR 值，如果您对 CIDR 这个概念还不熟悉，请仍然执行 `export POD_SUBNET=10.100.0.1/16` 命令，不做修改
- SERVICE_SUBNET service 所使用的网段不能与 master节点/worker节点 所在的网段重叠。

```bash
# 只在第一个 master 节点执行
# 替换 x.x.x.x 为 master 节点的内网IP
# export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令
# export MASTER_IP=x.x.x.x
export MASTER_IP=127.0.0.1
# 替换 apiserver.demo 为 您想要的 dnsName
export APISERVER_NAME=apiserver.demo
# Kubernetes 容器组所在的网段，该网段安装完成后，由 kubernetes 创建，事先并不存在于您的物理网络中
# pod 网段
export POD_SUBNET=10.100.0.1/16
# service 网段
export SERVICE_SUBNET=10.96.0.0/16
echo "${MASTER_IP}    ${APISERVER_NAME}" >> /etc/hosts

# 安装 1.18.4
# 请将脚本最后的 1.18.4 替换成您需要的版本号
curl -sSL https://xinlichao.cn/resource/back-end/k8s/k8s-0-1/init_master.sh | sh -s 1.18.4

# 执行结果中：
# 第15、16、17行，用于初始化第二、三个 master 节点
# 第25、26行，用于初始化 worker 节点
```

<pre style="display:none;">
<!--
* 如果出错看这里
* 请确保您使用 root 用户执行初始化命令
* 不能下载 kubernetes 的 docker 镜像
  * 安装文档中，默认使用阿里云的 docker 镜像仓库，然而，有时候，该镜像会罢工
  * 如碰到不能下载 docker 镜像的情况，请尝试手工初始化，并修改手工初始化脚本里的第22行（文档中已高亮）为：
    ```yaml
    imageRepository: gcr.azk8s.cn/google-containers
    ```
* 检查环境变量，执行如下命令
  ``` sh
  echo MASTER_IP=${MASTER_IP} && echo APISERVER_NAME=${APISERVER_NAME} && echo POD_SUBNET=${POD_SUBNET}
  ```
  请验证如下几点：
  * 环境变量 ***MASTER_IP*** 的值应该为 master 节点的 **内网IP**，如果不是，请重新 export
  * **APISERVER_NAME** 不能是 master 的 hostname
  * **APISERVER_NAME** 必须全为小写字母、数字、小数点，不能包含减号
  * **POD_SUBNET** 所使用的网段不能与 ***master节点/worker节点*** 所在的网段重叠。该字段的取值为一个 <a href="/glossary/cidr.html" target="_blank">CIDR</a> 值，如果您对 CIDR 这个概念还不熟悉，请仍然执行 export POD_SUBNET=10.100.0.1/16 命令，不做修改
* 重新初始化 master 节点前，请先执行 `kubeadm reset -f` 操
-->
</pre>

检查 master 初始化结果

```bash
# 只在 master 节点执行

# 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态
watch kubectl get pod -n kube-system -o wide

# 查看 master 节点初始化结果
kubectl get nodes -o wide
```

> 请等到所有容器组（大约9个）全部处于 Running 状态，才进行下一步

<pre style="display:none;">
<!--
* 如果出错看这里
* ImagePullBackoff / Pending
  * 如果 `kubectl get pod -n kube-system -o wide` 的输出结果中出现 ImagePullBackoff 或者长时间处于 Pending 的情况，请参考 [查看镜像抓取进度](/learning/faq/image-pull-backoff.html)
* ContainerCreating
  * 如果 `kubectl get pod -n kube-system -o wide` 的输出结果中某个 Pod 长期处于 ContainerCreating、PodInitializing 或 Init:0/3 的状态，可以尝试：
    * 查看该 Pod 的状态，例如：
      ``` sh
      kubectl describe pod kube-flannel-ds-amd64-8l25c -n kube-system
      ```
      如果输出结果中，最后一行显示的是 Pulling image，请耐心等待，或者参考 [查看镜像抓取进度](/learning/faq/image-pull-backoff.html)
      ```
      Normal  Pulling    44s   kubelet, k8s-worker-02  Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
      ```
    * 将该 Pod 删除，系统会自动重建一个新的 Pod，例如：
      ``` sh
      kubectl delete pod kube-flannel-ds-amd64-8l25c -n kube-system
      ```
-->
</pre>

### 初始化第二、三个master节点

> 在 demo-master-a-2 和 demo-master-a-3 机器上执行

```bash
# 只在第二、三个 master 节点 demo-master-a-2 和 demo-master-a-3 执行
# 替换 x.x.x.x 为 ApiServer LoadBalancer 的 IP 地址
export APISERVER_IP=x.x.x.x
# 替换 apiserver.demo 为 前面已经使用的 dnsName
export APISERVER_NAME=apiserver.demo
echo "${APISERVER_IP}    ${APISERVER_NAME}" >> /etc/hosts
# 使用前面步骤中获得的第二、三个 master 节点的 join 命令
kubeadm join apiserver.demo:6443 --token ejwx62.vqwog6il5p83uk7y \
--discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303 \
--control-plane --certificate-key 70eb87e62f052d2d5de759969d5b42f372d0ad798f98df38f7fe73efdf63a13c
```

> 如果一直停留在 pre-flight 状态，请在第二、三个节点上执行命令检查：
> curl -ik https://apiserver.demo:6443/version
> 如果没有返回200和版本号，请检查一下您的 Loadbalancer 是否设置正确

### 初始化 worker节点

#### 获得 join命令参数

> 在 master 节点上执行

``` sh
# 只在 master 节点执行
kubeadm token create --print-join-command
```

可获取kubeadm join 命令及参数，如下所示

``` sh
# kubeadm token create 命令的输出
kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303
```

> 该 token 的有效时间为 2 个小时，2小时内，您可以使用此 token 初始化任意数量的 worker 节点。

#### 初始化worker

> 针对所有的 worker 节点执行

``` sh
# 只在 worker 节点执行
# 替换 x.x.x.x 为 master 节点的内网 IP
export MASTER_IP=x.x.x.x
# 替换 apiserver.demo 为初始化 master 节点时所使用的 APISERVER_NAME
export APISERVER_NAME=apiserver.demo
echo "${MASTER_IP}    ${APISERVER_NAME}" >> /etc/hosts

# 替换为 master 节点上 kubeadm token create 命令的输出
kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303
```

<pre style="display:none;">
<!--
### 常见错误原因

经常在群里提问为什么 join 不成功的情况大致有这几种：

#### worker 节点不能访问 apiserver

  在worker节点执行以下语句可验证worker节点是否能访问 apiserver
  ``` sh
  curl -ik https://apiserver.demo:6443
  ```
  如果不能，请在 master 节点上验证
  ``` sh
  curl -ik https://localhost:6443
  ```
  正常输出结果如下所示：
  ``` {1}
  HTTP/1.1 403 Forbidden
  Cache-Control: no-cache, private
  Content-Type: application/json
  X-Content-Type-Options: nosniff
  Date: Fri, 15 Nov 2019 04:34:40 GMT
  Content-Length: 233

  {
    "kind": "Status",
    "apiVersion": "v1",
    "metadata": {
  ...
  ```
  ::: tip 可能原因
  * 如果 master 节点能够访问 apiserver、而 worker 节点不能，则请检查自己的网络设置
    * /etc/hosts 是否正确设置？
    * 是否有安全组或防火墙的限制？
  :::

#### worker 节点默认网卡
  
  * [Kubelet使用的 IP 地址](#检查网络) 与 master 节点可互通（无需 NAT 映射），且没有防火墙、安全组隔离
    * 如果你使用 vmware 或 virtualbox 创建虚拟机用于 K8S 学习，可以尝试 NAT 模式的网络，而不是桥接模式的网络

### 移除worker节点并重试

::: warning
正常情况下，您无需移除 worker 节点，如果添加到集群出错，您可以移除 worker 节点，再重新尝试添加
:::

在准备移除的 worker 节点上执行

``` sh
# 只在 worker 节点执行
kubeadm reset -f
```

在 master 节点 demo-master-a-1 上执行

```sh
# 只在 master 节点执行
kubectl get nodes -o wide
```
如果列表中没有您要移除的节点，则忽略下一个步骤

``` sh
# 只在 master 节点执行
kubectl delete node demo-worker-x-x
```

::: tip
* 将 demo-worker-x-x 替换为要移除的 worker 节点的名字
* worker 节点的名字可以通过在节点 demo-master-a-1 上执行 kubectl get nodes 命令获得
:::
-->
</pre>

## 安装 Ingress Controller

kubernetes支持多种Ingress Controllers (traefik / Kong / Istio / Nginx 等)，推荐使用 https://github.com/nginxinc/kubernetes-ingress

```bash
# 只在 master 节点执行
wget https://xinlichao.cn/resource/back-end/k8s/k8s-0-1/nginx-ingress.yaml-t -O nginx-ingress.yaml

# 安装
kubectl apply -f nginx-ingress.yaml

# 卸载
kubectl delete -f nginx-ingress.yaml
```

> 如果您打算将 Kubernetes 用于生产环境，请参考此文档 [Installing Ingress Controller](https://github.com/nginxinc/kubernetes-ingress/blob/v1.5.3/docs/installation.md)，完善 Ingress 的配置

## 在 IaaS 层完成如下配置（公网Load Balancer）

创建负载均衡 Load Balancer：

- 监听器 1：80 / TCP， SOURCE_ADDRESS 会话保持
- 服务器资源池 1： demo-worker-x-x 的所有节点的 80端口
- 监听器 2：443 / TCP， SOURCE_ADDRESS 会话保持
- 服务器资源池 2： demo-worker-x-x 的所有节点的443端口

假设刚创建的负载均衡 Load Balancer 的 IP 地址为： z.z.z.z

### 配置域名解析

将域名 *.demo.yourdomain.com 解析到地址负载均衡服务器 的 IP 地址 z.z.z.z

### 验证配置

在浏览器访问 a.demo.yourdomain.com，将得到 404 NotFound 错误页面

## 部署服务

### 使用私有镜像仓库


## 安装 Kubernetes dashboard

## 安装 Helm

## 安装 Istio

## 参考

- https://kubernetes.io/zh/docs/reference/kubectl/overview/
- https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/
- https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/
- https://kuboard.cn/
- https://kubectl.docs.kubernetes.io/
