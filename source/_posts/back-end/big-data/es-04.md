---
title: 重学 Elastic Stack 之 Elasticsearch NLP
date: 2021-02-03 23:00:00
categories: BigData
tags:
  - Elastic Stack
  - ES
---

人工智能、机器学习、深度学习、NLP 工具介绍。

<!--more-->

## 人工智能、机器学习、深度学习的关系

概括来说，人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的。人工智能是最宽泛的概念。机器学习是当前比较有效的一种实现人工智能的方式。深度学习是机器学习算法中最热门的一个分支，近些年取得了显著的进展，并替代了大多数传统机器学习算法。三者的关系即：人工智能 > 机器学习 > 深度学习。

![1][1]

**人工智能：**人工智能是一个非常大的概念，其最初定义是要让机器的行为看起来就像是人所表现出的智能行为一样。我们经常听到的语音识别、图像识别、自然语言理解等领域都是具体的人工智能方向，而机器学习、神经网络等概念都属于实现人工智能所需要的一些技术。

**机器学习：**机器学习是人工智能的一门分支，指通过学习过往经验来提升机器的智能性的一类方法。根据样本和训练的方式，又可以分为监督学习、无监督学习、半监督学习和强化学习等类型。

**神经网络：**在人工智能领域一般指人工神经网络，是一种模仿动物神经网络行为特征，进行分布式并行信息处理人工智能模型。我们通常使用的神经网络都需要通过训练数据进行参数的学习，所以神经网络也可以被归为一种机器学习方法。

**深度学习：**作为人工智能领域的新兴方向，深度学习目前还没有严格的定义，一般我们把一些具备较多中间隐含层的神经网络称为深度学习模型。

> 机器学习：一种实现人工智能的方法； 深度学习：一种实现机器学习的技术

### 深度学习改变了AI 应用的研发模式

深度学习改变了很多领域算法的实现模式。在深度学习兴起之前，很多领域建模的思路是投入大量精力做特征工程，将专家对某个领域的“人工理解”沉淀成特征表达，然后使用简单模型完成任务（如分类或回归）。而在数据充足的情况下，深度学习模型可以实现端到端的学习，即不需要专门做特征工程，将原始的特征输入模型中，模型可同时完成特征提取和分类任务。

![2][2]

### 深度学习的应用

- 计算机视觉：卷积神经网络（Convolutional Neural Networks, CNN）是计算机视觉技术最经典的模型结构，图片识别；
- 自然语言处理（Natural Language Processing，简称NLP），文字识别，语义理解；
- 推荐系统，视频平台的电影推荐，电商平台的商品推荐。

### 深度学习在电影推荐系统中的应用

如果能将用户A的原始特征转变成一种代表用户A喜好的特征向量，将电影1的原始特征转变成一种代表电影1特性的特征向量。那么，我们计算两个向量的相似度，就可以代表**用户A对电影1的喜欢程度**。据此，推荐系统可以如此构建：

![3][3]

假如给用户A推荐，计算电影库中“每一个电影的特征向量”与“用户A的特征向量”的余弦相似度，根据相似度排序电影库，取 Top K 的电影推荐给A。

### 深度学习框架

- TensorFlow：2015年11月 Google 研制，最流行的深度学习框架，社区强大；
- PyTorch：2016年10月 Facebook 研制，据说用过 TensorFlow 再用 PyTorch 都说香；
- PaddlePaddle(飞桨)：百度研制 2016年 开源，国内第一款深度学习框架，现已平台化；
- MXNet：2017年1月 MXNet 项目进入 Apache 基金会成为 Apache 的孵化器项目。

## 人工智能的目标

- 推理
- 自主学习 & 调度
- 机器学习
- 自然语言处理（NLP）
- 计算机视觉处理
- 机器人
- 通用智能

## 人工智能三大阶段

- 机器学习：只能系统使用一系列算法从经验中进行学习
- 机器智能：机器使用的一系列从经验中进行学习的高级算法，例如深度神经网络（深度学习），人工智能目前也处于现阶段
- 机器意识：不需要外部数据就能从经验中自我学习。

## 人工智能的类型

- 狭义人工智能（ANI）：它包含基础的、角色行任务。例如小爱、Siri、Alexa这样的聊天机器人，个人助手完成的任务。
- 通用人工智能（AGI）：通用人工智能包含人类水平的任务，它涉及到机器的持续学习。
- 强人工智（ASI）：强人工智能代指比人类更聪明的机器。

## 怎样让机器智能化

- 自然语言处理
- 知识表示
- 自动推理
- 机器学习

## NLP

### NLP的目标

NLP的目标是让计算机在理解语言方面像人类一样智能，最终的目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。

### 为什么需要NLP

有了NLP，就可能完成自动语音、自动文本的编写等任务。让我们从大量的数据中解放出来，让计算机去执行。
这些任务包括自动生成给定文本的摘要、机器翻译及其他的任务。

现在，有一个给定的文本。NLP可以从下面三个不同等级来分析该给定文本：

- 语法学：该文本语法的正确性
- 语义学：该文本的含义是什么
- 语用学：该文本的目的的什么

除此之外，如果文本中含有音、视频，那么NLP又要从以下两方面来着手分析：

- 音韵学：该语言中发音的系统化组织
- 词态学：研究单词构成以及彼此之间的关系

NLP中理解语义的方法：

- 分布式：利用机器学习和深度学习的大规模统计策略
- 框架式：句法不同，但语义相同的句子在数据结构被表示为程式化的情景
- 理论式：思路是句子指代的正真的词结合句子的部分内容可表达全部含义
- 交互式（学习）：它涉及大语用方法，在交互式学习环境中用户教计算机一步一步的学习语言

### NLP的流程

NLP的机制涉及两个流程：

- 自然语言理解
- 自然语言生成

### 自然语言理解（NLU）

自然语言理解（Nature Language Understanding）是要理解给定文本的含义。比如文本内的每个单词的特性与结构需要被理解。在理解结构的基础上，NLU要理解自然语言中以下几个歧义：

- 词法歧义性：单词有多重含义
- 句法歧义性：语句有多重解析树
- 语义歧义性：句子有多重含义
- 回指歧义性：前文中的词语或句子在后面句子中有不同的含义

### 自然语言生成（NLG）

NLG是从结构化数据中以可读的方式自动生成文本的过程。
自然语言生成可分为三个阶段：

- 文本规划：完成结构化数据中基础内容的规划
- 语句规划：从结构化数据中组合语句来表达信息流
- 实现：生产语法通顺的语句来表达文本

### NLP的应用领域

#### 目前，人工智能都做了什么？

- 语音识别：小米家的小爱同学，苹果的Siri，微软的cortana
- 语音合成：小米家的小爱同学，苹果的Siri，微软的cortana
- 图像识别：交通摄像头的违规拍照识别，刷脸解锁手机，指纹解锁
- 视频识别：抖音视频内容审核，视频社交App的审核机制
- 文字识别：身份证识别，银行卡识别，扫一扫翻译
- 语义理解：智能问答机器人，也包括上面的小爱同学、Siri、cortana

#### 我们身边的人工智能

- 银行卡办理刷卡就行
- 车辆违章，只要有牌就跑不了
- 天眼，违法犯罪路过天眼，等于自投罗网
- 语音助手之小爱同学，“小爱同学”，“哎”，“打开电视”，“好的”，“kua，洗衣机就开始转了”
- 联通10010智能语音系统，“我还有多少话费？”，“您已欠费！”
- 扫一扫翻译看不懂的文字。

#### 聊天机器人

聊天机器人或者智能代理指的是，你能通过APP、聊天窗口、语音等方式进行交流的计算机程序。
它的重要性在越来越多的地方得到体现：

- 它对理解数字化客服和频繁咨询的常规问答领域中的变化至关重要
- 它在一些特定场景下非常的有用及高效，特别是会被频繁问到的高度可预测的问题时

聊天机器人的工作机制：

- 基于知识：包含信息库，根据客户的问题回应相对的问题
- 数据存储：包含与用户交流的历史信息
- NLP层：该层将用户的问题转译为信息，从而作出合适的回应
- 应用层：用来与用户交互的应用接口

NLP中深度学习的重要性

- 它使用基于规则的方法将单词表示为`one-hot`编码向量
- 传统的方法注重句法表征，而非语义表征
- 词袋，分类模型不能够分别特定语境

深度学习的三项能力：

- 可表达性：该能力描述了机器如何能近似通用函数
- 可训练性：深度学习系统学习问题的速度与能力
- 可泛化性：在未训练过的数据上，机器做预测的能力

除此之外，深度学习还有其他的能力，比如可解释性、模块性、可迁移性、延迟、对抗稳定性、安全方面等。

#### 日志中的NLP

在日志分析和日志挖掘两方面，NLP在发挥着不可替代的作用。通过词语切分、词干提取、词形还原、解析等不同技术被用来将日志转换成结构化的形式。
在日志分析中，NLP通过下列技术完成分析功能：

- 模式识别：将日志信息与模式薄中的信息进行对比，从而过滤信息的技术
- 标准化：日志信息的标准化将不同的信息转换为同样的格式。当不同来源的日志信息中有不同的疏于，但其含义相同时，需要进行标准化
- 分类& 标签：不同日志信息的分类 & 标签涉及到对信息的排序，并用不同的关键词进行标注
- Artificial Ignorance：使用机器学习算法抛弃无用日志信息的技术。它也可被用来检测系统异常

当日志以很好的形式组织起来之后，我们就能从日志中提取有用的信息。

#### NLP的其他领域

除了在大数据、日志挖掘和分析中，NLP还浪迹在其他的应用领域中：

- 自动摘要：在给定文本的情况下，摒弃次要信息完成文本摘要
- 情感分析：在给定文本中预测期主题，比如文本中是否包含批判、观点、评论等
- 文本分类：按照其领域分类不同的、新闻报道、期刊等。比如流行的文本分类是垃圾邮件、基于写作风格可检测作者的姓名
- 信息提取：建议电子邮件程序自动添加事件到日历

## 自然语言处理之中文分词器

中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块。不同于英文的是，中文句子中没有词的界限，因此在进行中文自然语言处理时，通常需要先进行分词，分词效果将直接影响词性、句法树等模块的效果。当然分词只是一个工具，场景不同，要求也不同。
在人机自然语言交互中，成熟的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。
根据中文分词实现的原理和特点，可以分为：

- 基于词典分词算法
- 基于理解的分词方法
- 基于统计的机器学习算法

### 基于词典分词算法

基于词典分词算法，也称为字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已经建立好的"充分大的"词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法为一下几种：

- 正向最大匹配算法。
- 逆向最大匹配法。
- 最少切分法。
- 双向匹配分词法。

基于词典的分词算法是应用最广泛，分词速度最快的，很长一段时间内研究者在对对基于字符串匹配方法进行优化，比如最大长度设定，字符串存储和查找方法以及对于词表的组织结构，比如采用TRIE索引树，哈希索引等。

这类算法的优点：速度快，都是O(n)的时间复杂度，实现简单，效果尚可。

算法的缺点：对歧义和未登录的词处理不好。

### 基于理解的分词方法

这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果，其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象，它通常包含三个部分：分词系统，句法语义子系统，总控部分，在总控部分的协调下，分词系统可以获得有关词，句子等的句法和语义信息来对分词歧义进行判断，它模拟来人对句子的理解过程，这种分词方法需要大量的语言知识和信息，由于汉语言知识的笼统、复杂性，难以将各种语言信息组成及其可以直接读取的形式，因此目前基于理解的分词系统还在试验阶段。

### 基于统计的机器学习算法

这类目前常用的算法是HMM，CRF，SVM，深度学习等算法，比如stanford，Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备良好的学习能力，因此对歧义词和未登录词的识别都具有良好的效果。

Nianwen Xue在其论文中《[Combining Classifier for Chinese Word Segmentation](http://www.aclweb.org/anthology/W/W02/W02-1815.pdf)》中首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，在论文《[Chinese word segmentation as character tagging](https://www.aclweb.org/anthology/O03-4002)》中较为详细地阐述了基于字标注的分词法。

算法优点：能很好处理歧义和未登录词问题，效果比前一类效果好

算法缺点: 需要大量的人工标注数据，以及较慢的分词速度

### 分词器目前存在的问题

目前中文分词难点主要有三个：

- 分词标准：比如人名，在哈工大的标准中姓和名是分开的，但是在Hanlp中是合在一起的，这需要根据不同的需求制定不同的分词标准。
- 歧义：对于同一个待切分字符串存在多个分词结果。歧义又分为组合歧义，交集型歧义和真歧义三种分类。
  - 组合型歧义：分词是有不同的粒度的，指某个词条中的一部分也可以切分未一个独立的词条，比如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是`中华/人民/共和国`
  - 交集型歧义：在`郑州天和服装厂`中，`天和`是厂名，是一个专有名词，`和服`也是一个词，它们共用了`和`字
  - 真歧义：本身的语法和语义都没有问题，即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果，例如：对于句子`美国会通过对台售武法案`，既可以切分成`美国/会/通过台售武法案`也可以切分成`美/国会/通过台售武法案`。
- 新词：也称未被词典收录的词，该问题的解决依赖于人们对分词技术和汉语语言结构进一步认识。

一般在搜索引擎中,构建索引时和查询时会使用不同的分词算法，常用的方案是，在索引的时候，使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。

## 分词工具介绍

- 中科院计算所[NLPIR](http://ictclas.nlpir.org/nlpir/html/jianjie-0.html)：NLPIR能够全方位多角度满足应用者对大数据文本的处理需求，包括大数据完整的技术链条：网络抓取、正文提取、中英文分词、词性标注、实体抽取、词频统计、关键词提取、语义信息抽取、文本分类、情感分析、语义深度扩展、繁简编码转换、自动注音、文本聚类等。
  NLPIR所有功能模块全部备有对应的二次开发接口（动态链接库.dll，.so，及静态链接库等形式），平台的各个中间件API可以无缝地融合到客户的各类复杂应用系统之中，可兼容Windows，Linux， Android，Maemo5, FreeBSD，麒麟等不同操作系统，开发者可使用Java，C/C++，C#, Python，Php, R等各类主流开发语言调用其所有功能。
- 哈工大的[LTP](https://github.com/HIT-SCIR/ltp)：语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。
- [ansj分词器](https://github.com/NLPchina/ansj_seg)：这是一个基于n-Gram+CRF+HMM的中文分词的java实现；分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上；目前实现了中文分词、 中文姓名识别 、 用户自定义词典、关键字提取、自动摘要、关键字标记等功能。可以应用到自然语言处理等方面,适用于对分词效果要求高的各种项目。
- 清华大学[THULAC](https://github.com/thunlp/THULAC)：THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：
  - 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
  - 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。
  - 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。
- [斯坦福分词器](https://nlp.stanford.edu/software/segmenter.shtml：)：作为众多斯坦福自然语言处理中的一个包， Java实现的CRF算法。可以直接使用训练好的模型，也提供训练模型接口。
- [Hanlp分词器](https://github.com/hankcs/HanLP)：求解的是最短路径。优点：开源、有人维护、可以解答。原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。
- [结巴分词](https://github.com/yanyiwu/cppjieba)：基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。
- [KCWS分词器(字嵌入+Bi-LSTM+CRF)](https://github.com/koth/kcws)：本质上是序列标注，这个分词器用人民日报的80万语料，据说按照字符正确率评估标准能达到97.5%的准确率，各位感兴趣可以去看看。
- [ZPar](https://github.com/frcchang/zpar/releases)：新加坡科技设计大学开发的中文分词器，包括分词、词性标注和Parser，支持多语言，据说效果是公开的分词器中最好的，C++语言编写。
- [IKAnalyzer](https://github.com/wks/ik-analyzer)：IKAnalyzer是一个开源的，基于java的语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始，IKAnalyzer已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer3.0则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。

## 参考

- https://www.paddlepaddle.org.cn/tutorials/projectdetail/931981

[1]: /images/big-data/es-04/1.png
[2]: /images/big-data/es-04/2.png
[3]: /images/big-data/es-04/3.jpg
