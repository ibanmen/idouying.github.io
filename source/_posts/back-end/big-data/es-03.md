---
title: 重学 Elastic Stack 之 Elasticsearch 入门
date: 2021-02-01 22:00:00
categories: BigData
tags:
  - Elastic Stack
  - ES
---

Elasticsearch 是使用 Java 编写的一种开源搜索引擎，它在内部使用 Luence 做索引与搜索，通过对 Lucene 的封装，提供了一套简单一致的 RESTful API。

Elasticsearch 也是一种分布式的搜索引擎架构，可以很简单地扩展到上百个服务节点，并支持 PB 级别的数据查询，使系统具备高可用和高并发性。

本文介绍 Elasticsearch 基本概念和常用REST API。

<!--more-->

## 基本概念 - 索引、文档和REST API

**Dev 视角**

- Index 索引
  - Type 类型
  - Document 文档

**Ops 视角**

- Node 节点
  - Shard 分片

![1][1]

### 文档（Document）

- Elasticsearch 是面向文档的，文档是所有可搜索数据的最小单位
  - 日志文件中的日志项
  - 一本电影的具体信息 /一张唱片的详细信息
  - MP3 播放器里的一首歌 /一篇 PDF 文档中的具体内容
- 文档会被序列化成 JSON格式，保存在 Elasticsearch 中
  - JSON 对象由字段组成，
  - 每个字段都有对应的字段类型（字符串/数值/布尔/日期/二进制/范围类型）
- 每个文档都有一个 Unique ID。
  - 你可以自己指定 ID
  - 或者通过 Elasticsearch 自动生成

### JSON 文档

- 一篇文档包含了一系列的字段。类似数据库表中一条记录
- JSON 文档，格式灵活，不需要预先定义格式
  - 字段的类型可以指定或者通过 Elasticsearch 自动推算
  - 支持数组 / 支持嵌套

![2][2]

### 文档的元数据

<img src="/images/big-data/es-03/3.jpg" align="right" style="zoom:50%;" />

- 元数据，用于标注文档的相关信息
  - _index -文档所属的索引名
  - _type - 文档所属的类型名
  - _id -文档唯一 Id
  - _source∶ 文档的原始 Json 数据
  - _all∶ 整合所有字段内容到该字段，已被废除
  - _version∶ 文档的版本信息
  - _score∶ 相关性打分

<!-- ![3][3] -->

### 索引

<img src="/images/big-data/es-03/4.jpg" align="right" style="zoom:50%;" />

- Index-索引是文档的容器，是一类文档的结合
  - Index 体现了逻辑空间的概念∶每个索引都有自己的 Mapping 定义，用于定义包含的文档的字段名和字段类型
  - Shard 体现了物理空间的概念∶索引中的数据分散在 Shard上
- 索引的 Mapping 与 Settings
  - Mapping 定义文档字段的类型
  - Setting 定义不同的数据分布

<!-- ![4][4] -->

#### 索引的不同语意

<img src="/images/big-data/es-03/5.jpg" align="right" style="zoom:50%;" />

- 名词：一个 Elasticsearch 集群中，可以创建很多个不同的索引
- 动词：保存一个文档到 Elasticsearch 的过程也叫索引(indexing)
  - ES中，创建一个倒排索引的过程
- 名词：一个 B 树索引，一个倒排索引

<!-- ![5][5] -->

### Type

- 在 7.0 之前，一个索引 Index 可以设置多个 Types
- 6.0 开始，Type 已经被 Deprecated。7.0 开始，一个索引智能创建 一个 Type - “_doc”

![6][6]

> 使用类型的初衷是在与 Lucene 不兼容的单个索引中提供多租户，但遗憾的是，事实证明，类型带来的问题比解决的问题还多。https://www.elastic.co/cn/blog/moving-from-types-to-typeless-apis-in-elasticsearch-7-0

### 抽象与类比

| RDBMS  | Elasticsearch |
| :----: | :-----------: |
| Table  |  Index(Type)  |
|  Row   |   Document    |
| Column |     Filed     |
| Schema |    Mapping    |
|  SQL   |      DSL      |

传统关系型数据库和 Elasticsearch 的区别

- Elasticsearch - Schemaless / 相关性 / 高性能全文检索
- RDBMS - 事务性 / Join

### REST API - 很容易被各种语言调用

![7][7]

## 基本概念 - 集群、节点、分片、副本

### 分布式系统的可用性与扩展性

- 高可用性
  - 服务可用性 - 允许有节点停止服务
  - 数据可用性 - 部分节点丢失，不会丢失数据
- 可扩展性
  - 请求量提升 / 数据的不断增长（将数据分布到所有节点上）

### 分布式特性

- Elasticsearch 的分布式架构的好处
  - 存储的水平扩容
  - 提高系统的可用性，部分节点停止服务，整个集群的服务不受影响
- Elasticsearch 的分布式架构
  - 不同的集群通过不同的名字来区分，默认名字"elasticsearch"
  - 通过配置文件修改，或者在命令行中 `-E cluster.name=demo` 进行设定
  - 一个集群可以有一个或者多个节点

### 节点

- 节点是一个 Elasticsearch 的实例
  - 本质上就是一个 JAVA 进程
  - 一台机器上可以运行多个 Elasticsearch 进程，但是生产环境一般建议一台机器上只运行一个 Elasticsearch 实例
- 每一个节点都有名字，通过配置文件配置，或者启动时候 `-Enode.name=node1` 指定
- 每一个节点在启动之后，会分配一个 UID，保存在 data 目录下

### Master-eligible nodes 和 Master Node

- 每个节点启动后，默认就是一个 Master eligble 节点
  - 可以设置 `node.master∶ false` 禁止
- Master-eligible节点可以参加选主流程，成为 Master 节点
- 当第一个节点启动时候，它会将自己选举成 Master 节点
- 每个节点上都保存了集群的状态，只有 Master 节点才能修改集群的状态信息
  - 集群状态（Cluster State），维护了一个集群中，必要的信息
    - 所有的节点信息
    - 所有的索引和其相关的 Mapping 与 Setting 信息
    - 分片的路由信息
  - 任意节点都能修改信息会导致数据的不一致性

### Data Node & Coordinating Node

- Data Node
  - 可以保存数据的节点，叫做 Data Node。负责保存分片数据。在数据扩展上起到了
至关重要的作用
- Coordinating Node
  - 负责接受Client的请求，将请求分发到合适的节点，最终把结果汇集到一起
  - 每个节点默认都起到了 Coordinating Node的职责

### 其他的节点类型

- Hot & Warm Node
  - 不同硬件配置的 Data Node，用来实现 Hot & Warm 架构，降低集群部署的成本
- Machine Learning Node
  - 负责跑 机器学习的 Job，用来做异常检测
- Tribe Node
  - （5.3 开始使用 Cross Cluster Serarch）Tribe Node 连接到不同的 Elasticsearch集群，
并且支持将这些集群当成一个单独的集群处理

### 配置节点类型

- 开发环境中一个节点可以承担多种角色
- 生产环境中，应该设置单一的角色的节点（dedicated node）

|     节点类型      |  配置参数   |                                    默认值                                     |
| :---------------: | :---------: | :---------------------------------------------------------------------------: |
|  maste eligible   | node.master |                                     true                                      |
|       data        |  node.data  |                                     true                                      |
|      ingest       | node.ingest |                                     true                                      |
| coordinating only |     无      | 每个节点默认都是 coordinating 节点。coordinating only 设置其他类型全部为false |
| machine learning  |   node.ml   |                            true（需enablex-pack）                             |

### 分片（Primary Shard & Replica Shard）

- 主分片，用以解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内的所有节点之上
  - 一个分片是一个运行的 Lucene 的实例
  - 主分片数在索引创建时指定，后续不允许修改，除非 Reindex
- 副本，用以解决数据高可用的问题。分片是主分片的拷贝
  - 副本分片数，可以动态题调整
  - 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）
- 一个三节点的集群中，blogs 索引的分片分布情况
  - 思考：增加一个节点或改大主分片数对系统的影响？

![8][8]

### 分片的设定

- 对于生产环境中分片的设定，需要提前做好容量规划
  - 分片数设置过小
    - 导致后续无法增加节点实现水品扩展
    - 单个分片的数据量太大，导致数据重新分配耗时
  - 分片数设置过大，7.0 开始，默认主分片设置成1，解决了over-sharding的问题
    - 影响搜索结果的相关性打分，影响统计结果的准确性
    - 单个节点上过多的分片，会导致资源浪费，同时也会影响性能

### ES 提供 API ，了解集群情况

- Green - 主分片跟副本都正常分配
- Yellow - 主分片全部分配正常，有副本分片问能正常分配
- Red - 有主分片未能分配
  - 例如：有磁盘容量超过 85% 时，去创建新的索引

```json
GET _cluster/health //查看健康
{
  "cluster_name" : "demo-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 2,
  "active_shards" : 4,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
```

## 文档的基本 CRUD 与批量操作

### 文档的 CRUD 操作

|  操作  | 请求                                                                                                                                                                             |
| :----: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Index  | PUT my_index/_doc/1 <br/> {"user":"mike","comment":"You know,for search"}                                                                                                        |
| Create | PUT my_index/_create/1 <br/> {"user":"mike","comment":"You know,for search"} <br/> POST my_index/_doc (不指定ID，自动生成) <br/> {"user":"mike","comment":"You know,for search"} |
|  Read  | GET my_index/_doc/1                                                                                                                                                              |
| Update | POST my_index/_update/1 <br/> { "doc": {"user":"mike","comment":"You know,Elasticsearch"}}                                                                                       |
| Delete | DELETE my_index/_doc/1                                                                                                                                                           |

- Type 名，约定都用 _doc
- Create - 如果ID已经存在，会失败
- Index - 如果ID不存在，创建新的文档。否则，先删除现有的文档，再创建新的文档，版本会增加
- Update - 文档必须已经存在，更新只会对相应字段做增量修改

### Bulk API

- 支持在一次 API 调用中，对不同的索引进行操作
- 支持四种类型操作
  - Index
  - Create
  - Update
  - Delete
- 可以再 URI 中指定 Index，也可以在请求的 Payload中进行
- 操作中单条操作失败，并不会影响其他操作
- 返回结果包括了每一条操作执行的结果

```json
POST _bulk
{ "index" : { "_index" : "test", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_id" : "2" } }
{ "create" : { "_index" : "test2", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }
```

#### Bulk 请求体格式

**Bulk 请求格式：**

```json
{"action": {"meta"}}\n
{"data"}\n
{"action": {"meta"}}\n
{"data"}\n
```

为什么不使用标椎JOSN格式？

```json
[
  {
    "action":{},
    "data":{}
  }
]
```

> 原因是与底层性能优化关系：bulk 中的每个操作都可能要转发到不同的 node 的 shard 去执行

**ES处理标准格式JSON串的流程：**

使用良好的json数组格式，允许任意换行，可读性非常好，但是es拿到那种标准格式的json串以后，要按照下述流程去进行处理

1. 将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象
2. 解析json数组里的每个json，对每个请求中的document进行路由
3. 为路由到同一个shard上的多个请求，创建一个请求数组
4. 将这个请求数组序列化
5. 将序列化后的请求数组发送到对应的节点上去

**Bulk 使用标准格式JSON串的弊端：**

Bulk size最佳大小，一般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为JsonArray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成JsonArray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。

占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降。

另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，更频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多。

**Bulk 奇特格式的好处：**

1. 不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json
2. 对每两个一组的json，读取meta，进行document路由
3. 直接将对应的json发送到node上去
4. 最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能

#### Bulk 一次最大处理多少数据量

bulk 会把将要处理的数据载入内存中，所以数据量是有限制的，最佳的数据量不是一个确定的数值，它取决于你的硬件，你的文档大小以及复杂性，你的索引以及搜索的负载.

一般建议是1000-5000个文档，如果你的文档很大，可以适当减少队列,大小建议是5-15MB，默认不能超过100M，可以在es的配置文件中修改这个值 `http.max_content_length: 100mb`

### 批量读取 - mget

批量操作，可以减少网络连接所产生的开销，提高性能

```json
GET /_mget
{
  "docs" : [
    {
      "_index" : "user",
      "_id" : "1"
    },
    {
      "_index" : "user",
      "_id" : "2"
    },
    {
      "_index" : "comment",
      "_id" : "1"
    }
  ]
}

// 指定索引
GET /user/_mget
{
  "docs" : [
    {
      "_id" : "1"
    },
    {
      "_id" : "2"
    }
  ]
}
```

### 批量查询 - msearch

对不同的索引，进行不同的 search

```json
// 需要通过 Kibana 导入Sample Data的电商数据
POST kibana_sample_data_ecommerce/_msearch
{}
{"query" : {"match_all" : {}},"size":1}
{"index" : "kibana_sample_data_flights"}
{"query" : {"match_all" : {}},"size":2}
```

### 常见错误返回

|     问题     |                     原因                     |
| :----------: | :------------------------------------------: |
|   无法连接   |             网络故障或者集群挂了             |
| 连接无法关闭 |             网络故障或者节点出错             |
|     429      | 集群过于繁忙（重试或者增加节点已增加吞吐量） |
|     4xx      |                请求体格式有误                |
|     500      |                 集群内部错误                 |

## ES 核心 - 全文检索

在我们生活中的数据总体是分为两种的：结构化数据和非结构化数据。

- 结构化数据：指具有固定格式或有限长度的数据，如数据库，元数据等；
- 非结构化数据：指不定长或无固定格式的数据，如邮件，word文档等磁盘上的文件。

如果是结构化数据，用数据库中的搜索很容易实现，因为数据库中的数据存储是有规律的，有行有列而且数据格式、数据长度都是固定的。

### **非结构化数据查询有两种办法：**

- **顺序扫描法**(Serial Scanning)
- 所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。

- **全文检索**(Full-text Search)
- 将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。这部分从非结构化数据中提取出的然后重新组织的信息，我们称之**索引**。
  - 例如：字典，字典的拼音表和部首检字表就相当于字典的索引，对每一个字的解释是非结构化的，如果字典没有音节表和部首检字表，在茫茫辞海中找一个字只能顺序扫描。然而字的某些信息可以提取出来进行结构化处理，比如读音，就比较结构化，分声母和韵母，于是将读音拿出来按一定的顺序排列，每一项读音都指向此字的详细解释的页数。我们搜索时按结构化的拼音搜到读音，然后按其指向的页数，便可找到我们的非结构化数据——也即对字的解释。

> **这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search)。**

## ES 核心 - 倒排索引

### **目录 - 正排索引**

![9][9]

### **索引页 - 倒排索引**

![10][10]

### **图书和搜索引擎的类比**

- 图书
  - 正排索引 - 目录页
  - 倒排索引 - 索引页
- 搜索引擎
  - 正排索引 - 文档ID 到文档内容和单词的关联
  - 倒排索引 - 单词到文档 ID 的关系

![11][11]

### **ES 倒排索引的核心组成**

- 倒排索引包含两个部分
  - 单词词典（Term Dictionary），记录所有文档的单词，记录单词到倒排列表的关联关系
    - 单词词典一般比较大，可以通过B＋树或哈希拉链法实现，以满足高性能的插入与查询
  - 倒排列表（Posting List）- 记录了单词对应的文档结合，由倒排索引项组成
    - 倒排索引项（Posting）
      - 文档 ID
      - 词频 TF - 该单词在文档中出现的次数，用于相关性评分
      - 位置（Position）- 单词在文档中分词的位置。用于语句搜索（phrase query）
      - 偏移（Offset）- 记录单词的开始结束位置，实现高亮显示

![一个ES例子][12]

### **Elasticsearch 的倒排索引**

- Elasticsearch 的 JSON 文档中的每个字段，都有自己的倒排索引
- 可以指定对某些字段不做索引
  - 优点：节省储存空间
  - 缺点：字段无法被搜索

## ES 核心 - 通过 Analyzer 进行分词

- Analysis  - 文本分析是把全文本转换一系列单词（term / token）的过程，也叫分词
- Analysis 是通过 Analyze 实现
  - 可以使用 ES 内置的分析器或者按需定制化分析器
- 除了在数据写入时转换词条，匹配 Query 语句时候也需要用相同的分析器对查询语句进行分析

### **Analyze 的组成**

- 除了在数据写入时转换词条，匹配 Query 语句时候也需要用相同的分析器对查询语句进行分析
  - Character Filters（针对原始文本处理，例如去除html）
  - Tokenizer（按照规则切分为单词）
  - Token Fiter（将切分的的单词进行加工，小写，删除 stopwords，增加同义词）

  ![13][13]

### **Elasticsearch 的内置分词器**

- Standard Analyzer - 默认分词器，按词切分，小写处理；
- Simple Analyzer - 按照非字母切分（符号被过滤），小写处理；
- Stop Analyzer - 小写处理，停用词过滤（the，a，is）；
- Whitespace Analyzer - 按照空格切分，不转小写；
- Keyword Analyzer - 不分词，直接将输入当作输出；
- Patter Analyzer - 正则表达式，默认\W+（非字符分隔）；
- Language - 提供了30多种常见语言的分词器；
- Customer Analyzer 自定义分词器。

### 使用 _analyzer API

直接指定 Analyzer 进行测试

```json
GET _analyze
{
  "analyzer": "standard",
  "text" : "Mastering Elasticsearch, elasticsearch in Action"
}
```

指定索引的字段进行测试

```json
POST books/_analyze
{
  "field": "title",
  "text": "Mastering Elasticesearch"
}
```

自定义分词进行测试

```json
POST /_analyze
{
  "tokenizer": "standard", 
  "filter": ["lowercase"],
  "text": "Mastering Elasticesearch"
}
```

### Standard Analyzer

<img src="/images/big-data/es-03/14.jpg" align="right" style="zoom:50%;" />

- 默认的分词器
- 按词切分
- 小写处理

```json
GET _analyze
{
  "analyzer": "standard",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}    
```

<!-- ![14][14] -->

### Simple Analyzer

- 按照非字母切分，非字母的都被去除
- 小写处理

```json
// simple 去除非字母的 ：2 - xi
GET _analyze
{
  "analyzer": "simple",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

![15][15]

### Whitespace Analyzer

<img src="/images/big-data/es-03/16.jpg" align="right" style="zoom:50%;" />

- 按照空格切分

```json
GET _analyze
{
  "analyzer": "whitespace",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

<!-- ![16][16] -->

### Stop Analyzer

<img src="/images/big-data/es-03/17.jpg" align="right" style="zoom:50%;" />

- 相比 Simple Analyzer
- 多了 stop filter
  - 后把 the, a, is, in 等修饰性词语去除

```json
GET _analyze
{
  "analyzer": "stop",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

<!-- ![17][17] -->

### Keyword Analyzer

<img src="/images/big-data/es-03/18.jpg" align="right" style="zoom:50%;" />

- 不分词，直接将输入当作一个 term 输出

```json
GET _analyze
{
  "analyzer": "keyword",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

<!-- ![18][18] -->

### Pattern Analyzer

<img src="/images/big-data/es-03/19.jpg" align="right" style="zoom:50%;" />

- 通过正则表达进行分词
- 默认是 \W+，非字符的符号进行分隔

```json
GET _analyze
{
  "analyzer": "pattern",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

<!-- ![19][19] -->

### Language Analyzer

- 各国语言分词

```json
GET _analyze
{
  "analyzer": "english",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}
```

### 中文分词的难点

- 中文句子，切分成一个一个次（不是一个个字）
- 英文中，单词有自然的空格作为分隔
- 一句中文，在不同的上下文，有不同的理解
  - 这个苹果，不大好吃 / 这个苹果，不大，好吃！
- 一些例子
  - 他说的确实在理 / 这事的确定不下来

### ICU Analyzer

<img src="/images/big-data/es-03/20.jpg" align="right" style="zoom:50%;" />

- 需要安装 plugin
  - Elasticsearch-plugin install analysis
- 提过了 Unicode 的支持，更好的支持亚洲语言！

```json
POST _analyze
{
  "analyzer": "icu_analyzer",
  "text": "他说的确实在理"
}
```

<!-- ![20][20] -->

### 更多的中文分词器

- IK
  - 支持自定义词库，支持热更新分词字典
  - https://github.com/medcl/elasticsearch-analysis-ik
- HanLP 分词器
  - 开源，社区活跃，原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。
  - https://www.hanlp.com

## ES 核心 - Search API

- URL Search
  - 在 URL 中使用查询参数
- Request Body Search
  - 使用 Elasticsearch 提供的，基于 JSON 格式的格式更加完备的 Query Dpmain Specific Language (DSL)

### 指定查询的索引

| 语法                   | 范围                |
| :--------------------- | :------------------ |
| /_search               | 集群上所有的索引    |
| /index1/_search        | index1              |
| /index1,index2/_search | index1 和 index2    |
| /index*/_search        | 以 index 开头的索引 |

### URI 查询

- 使用 “q”, 指定查询字符串
- “query string syntax”, KV 键值对

![21][21]

### Request Body

![22][22]

### 搜索 Response

![23][23]

### 搜索的相关性 Relevance

- 搜索是用户和搜索引擎的对话
- 用户关心的是搜索结果的相关性
- 是否找到所有相关的内容
  - 有多少不相关的内容被返回了
  - 文档的打分是否合理
  - 结合业务需求，平衡结果排名

![24][24]

**WEB 搜索**

- Page Rank 算法
  - 不仅仅是内容
  - 更重要的是内容的可信度

**电商搜索**

- 搜索引擎扮演 - 销售的角色
  - 提高用户购物体验
  - 提升网站的销售业绩
  - 去库存

**衡量相关性**

- Information Retrieval
  - Precision（查准率） - 尽可能返回较少的无关文档
  - Recall（查全率） - 尽量返回较多的相关文档
  - Ranking - 是否能够按照相关度进行排序

**Precision & Recall**

- Prcision - True Positive / 全部返回的结果 （True and False Positives）
- Recall - True Positive / 返回应该返回的结果 （True positives + false Negtives）
- 使用 Elasticsearch 的查询和相关参数改善搜索的 Precision 和 Recall

![25][25]

## ES 核心 - URI Search 详解

### 通过 URI query 实现搜索

```json
GET /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s
{
  "profile": "true"
}
```

- q 指定查询语句，使用 Query String Syntax
- df 默认字段，不指定时
- Sort 排序 /from 和 size 用于分页
- Profile 可以查看查询时如何被执行的

### Query String Synctax

- 指定字段 vs 泛查询
  - q=title:2012 / q=2012

  ```json
  //只对title字段进行查询
  GET /movies/_search?q=2012&df=title

  //泛查询，正对_all ,所有字段
  GET /movies/_search?q=2012
  {
    "profile": "true"
  }

  //对自定字段进行查询  跟 df 等效
  GET /movies/_search?q=title:2012
  {
    "profile": "true"
  }
  ```

- Term vs Phrase
  - Beautiful Mind 等效于 Beautiful OR Mind
  - “Beautiful Mind”, 等效于 Beautiful AND Mind。Phrase 查询，还要求前后顺序保存一致

  ```json
  //使用引号。Phrase
  GET /movies/_search?q=title:"Beautiful Mind"
  {
  "profile": "true"
  }

  //查找美丽心灵，Mind为泛查询 
  // 意思就是说 title 是Term  查询 "Beautiful" ，对所有字段查询"Mind"
  GET /movies/_search?q=title:Beautiful Mind
  {
    "profile": "true"
  }
  ```

- 分组和引号
  - title:(Beautiful AND Mind)
  - title=”Beautiful Mind”

  ```json
  //分组，Bool 查询 type：BooleanQuery
  GET /movies/_search?q=title:(Beautiful Mind)
  {
    "profile": "true"
  }
  ```

- 布尔操作
  - AND / OR / NOT 或者 && / || / !
    - 必须大写
    - title:(matrix NOT reloaded)

  ```json
  // type：BooleanQuery
  // title 里面必须包括Beautiful 跟 Mind
  GET /movies/_search?q=title:(Beautiful AND Mind)
  {
      "profile": "true"
  }

  // type：BooleanQuery 
  //必须包括Beautiful 但不包括 Mind
  GET /movies/_search?q=title:(Beautiful NOT Mind)
  {
      "profile": "true"
  }

  // type：BooleanQuery
  //包括Beautiful必须有Mind
  GET /movies/_search?q=title:(Beautiful %2BMind)
  {
      "profile": "true"
  }
  ```

- 分组
  - \+ 表示 must
  - \- 表示 must_not
  - title:(+matrix -reloaded)

- 范围查询
  - 区间表示：[] 闭区间 ，{} 开区间
    - year:{2019 TO 2018}
    - year:[* TO 2018]

- 算数符号
  - year:>2010
  - year(>2010 && <=2018)
  - year:(+>2010 +<=2018)

  ```json
  //范围查询，区间写法  / 数学写法
  GET /movies/_search?q=year:>=1980
  {
    "profile": "true"
  }
  ```

- 通配符查询（通配符查询效率低，占用内容大，不建议使用。特别是放在最前面）
  - ？代表 1 个字符，* 代表 0 或多个字符
    - title:mi?d
    - title:be*

  ```json
  //通配符查询
  GET /movies/_search?q=ttile:b*
  {
    "profile": "true"
  }
  ```

- 正则表达
  - title:[bt]oy
- 模糊匹配与近似查询
  - title:befutifl~1
  - title:"lord rings" ~2

  ```json
  //模糊匹配 
  //用户输错,还能找到
  GET /movies/_search?q=ttile:beautifl~1
  {
    "profile": "true"
  }
  // 近似度匹配 可查出     Lord of the Rings
  GET /movies/_search?q=ttile:"Lord Rings" ~2
  {
    "profile": "true"
  }
  ```

## Request Body 跟 Query DSL 简介

Request Body 可以实现很多高阶查询，建议多用 Request Body 做检索。

### Request Body Search

- 将查询语句通过 HTTP Request Body 发送给 Elasticsearch
- Query DS

```json
POST /movies,404_idx/_search?ignore_unavailable=true
{
  "profile":true,
  "query":{
    "match_all" : {}  // 查询所有的文档
  }
}
```

### 分页

- From 从 0 开始 默认返回 10 个结果
- 获取靠后的翻页，成本较高

```json
POST /kibana_sample_data_ecommerce/_search
{
"from":10,
"size":20
"query":{
   "match_all":{}
}
```

### 排序

- 最好在 “数字型” 与 “日期型” 字段上排序
- 因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值

```json
GET kinaba_sample_data_ecommerce/_search
{
  "sort":[{"order_date":"desc}],
  "from":10,
  "size":5,
   "query":{
   "match_all":{}
}
```

### _source filtering

- 如果_source 没有存储，那就只返回匹配的文档元数据
- _source 支持使用通配符
- _source[“name* “,”desc*”]

![26][26]

### 脚本字段

- 能算出新的字段
- 用例：订单中有不同的汇率，需要结合汇率对订单价格进行排序

```json
GET kibana_sample_data_ecommerce/_search
{
  "script_fields": {
    "new_field": {
      "script": {
        "lang": "painless",
        "source": "doc['order_date'].value+'hello'"
      }
    }
  },
  "query": {
    "match_all": {}
  }
}
```

### 使用查询表达式 - Match

```json
POST movies/_search
{
  "query": {
    "match": {
      "title": "Last Christmas"  // 相当于 OR 可出现其中1个
    }
  }
}

POST movies/_search
{
  "query": {
    "match": {
      "title": {
        "query": "Last Christmas",
        "operator": "AND"
      }
    }
  }
}
```

### 短语搜索 - Match Phrase

```json
POST movies/_search
{
  "query": {
    "match_phrase": {
      "title": {
        "query": "one love",
        "slop": 1 //中间可以有一个其他词
      }
    }
  }
}
```

## Query String & Simple Query String

### Query String Query

- 类似 URI Query – 把查询条件放在 POST 里面

```json
// 准备工作
PUT /users/_doc/1
{
  "name":"Leo Xin",
  "about":"java, golang, node, swift, elasticsearch"
}

PUT /users/_doc/2
{
  "name":"Richard Li",
  "about":"Hadoop"
}

//query_string
POST users/_search
{
  "query": {
    "query_string": {
      "default_field": "name",
      "query": "Leo AND Xin"
    }
  }
}

//query string 支持分组查询多个字段
POST users/_search
{
  "query": {
    "query_string": {
      "fields":["name","about"],
      "query": "(Leo AND Xin) OR (Java AND Elasticsearch)"
    }
  }
}
```

### Simple Query String Query

- 类似 Query String , 但是会忽略错误的语法同时只支持部分查询语句
- 不支持 AND OR NOT , 但会当作字符串处理
- Term 之间默认的关系是 OR, 可以指定 Operator
- 支持 部分逻辑
  - \+ 替代 AND
  - \| 替代 OR
  - \- 替代 NOT

```json
// Simple Query 默认的operator是 Or
POST users/_search
{
  "query": {
    "simple_query_string": {
      "query": "Leo AND Xin",
      "fields": ["name"]
    }
  }
}

POST users/_search
{
  "query": {
    "simple_query_string": {
      "query": "Leo Xin",
      "fields": ["name"],
      "default_operator": "AND"
    }
  }
}
```

## Dynamic Mapping 和常见字段类型

### Mapping

- Mapping 类似数据库的 schema 的定义，作用如下
  - 定义索引中的字段名称
  - 定义字段的数据类型，例如字符串，数字，布尔……
  - 字段，倒排索引的相关配置，(Analyzed or Not Analyzed, Analyzer)
- Mapping 会把 JSON 文档映射成 Lucene 所需的扁平格式
  - 一个 Mapping 属于一个索引的 Type
  - 每个文档都属于一个 Type
  - 一个 Type 有一个 Mapping 定义
  - 7.0 开始，不需要在 Mapping 定义指定 type 信息

### 字段的数据类型

- 简单类型
  - Text / Keyword
  - Date
  - Integer / Floating
  - Boolean
  - IPv4 & IPv6
- 复杂类型 - 对象和嵌套对象
  - 对象类型 / 嵌套类型
- 特殊类型
  - geo_point & geo_shape / percolator

### Dynamic Mapping

- 在写入文档时候，如果索引不存在，会自动创建索引
- Dynamic Mapping 的机制，使得我们无需手动定义Mappings。 Elasticsearch 会自动根据文档信息，推算出字段的类型
- 但是有时候会推算的不对，例如地理位置信息
- 当类型如果设置不对时，会导致一些功能无法正常运行，例如 Range 查询

![27][27]

### 类型的自动识别

| JSON 类型 | Elasticsearch 类型                                |
| :-------- | :------------------------------------------------ |
| 字符串    | 1. 匹配日期格式设置成 Date                        |
|           | 2. 设置数字设置为 float 或者 long，该选项默认关闭 |
|           | 3. 设置为 Text, 并增加 keyword 子字段             |
| 布尔值    | boolean                                           |
| 浮点数    | float                                             |
| 整数      | long                                              |
| 对象      | Object                                            |
| 数组      | 由第一个非空数值的类型所决定                      |
| 空值      | 忽略                                              |

### 能否更改 Mapping 的字段类型

- 两种情况
  - 新增字段
    - Dynamic 设置为 true 时，一定有新增字段的文档写入，Mapping 也同时被更新
    - Dynamic 设为 false，Mapping 不会被更新，自增字段的数据无法被索引，但是信息会出现在_source 中
    - Dynamic 设置成 Strict 文档写入失败
  - 对已有字段，一旦已经有数据写入，就不在支持修改字段定义
    - Luene 实现的倒排索引，一旦生成后，就不允许修改
    - 如果希望改变字段类型，必须 Reindex API，重建索引
- 原因
  - 如果修改了字段的数据类型，会导致已被索引的属于无法被搜索
  - 但是如果是增加新的字段，就不会有这样的影响

### 控制 Dynamic Mappings

![28][28]

- 当 dynamic 被设置成 false 时候，存在新增字段的数据写入，该数据可以被索引，当时新增字段被废弃
- 当设置成 Strict 模式的时候，数据写入直接出错

```json
//默认Mapping支持dynamic，写入的文档加入新的字段
PUT dynamic_mapping_test/_doc/1
{
  "newField":"someValue"
}
//能被搜索到
POST dynamic_mapping_test/_search
{
  "query": {
    "match": {
      "newField": "someValue"
    }
  }
}
//修改为dynamic false
PUT dynamic_mapping_test/_mapping
{
  "dynamic":false
}
//新增anotherField 成功
PUT dynamic_mapping_test/_doc/10
{
  "anotherField":"someValue"
}
//重新去查询，但是anotherField 未被搜索到
POST dynamic_mapping_test/_search
{
  "query": {
    "match": {
      "newField": "someValue"
    }
  }
}
//查看mapping
GET dynamic_mapping_test/_mapping

//修改为dynamic strict
PUT dynamic_mapping_test/_mapping
{
  "dynamic": "strict"
}
//新增newField 报错
PUT dynamic_mapping_test/_doc/12
{
  "newField":"value"
}
```

> PS: Mapping中的字段一旦设定后，禁止直接修改。因为倒排索引生成后不允许直接修改。需要重新建立新的索引，做reindex操作，修改之前的数据的参数值 还是可以的.

## 显示 Mapping 设置与常见参数

```json
PUT movies
{
  "mappings" : {
    // define your mappings here
  }
}
```

### 自定义 Mapping 的一些建议

- 可以参考 API 手册，纯手写
- 为了减少输入的工作量，减少出错率，依照以下步骤
  - 创建一个临时的 index，写入一些样本数据
  - 通过访问 Mapping API 获得该临时文件的动态 Mapping 定义
  - 修改后用，使用该配置创建的索引
  - 删除临时索引

### 控制当前字段是否被索引

- index - 控制当前字段是否被索引。默认为 true。如果设置成 false，该字段不可被搜索。

```json
PUT users
{
  "mappings" : {
    "properties" : {
      "firstName" : {
        "type" : "text"
      },
      "lastName" : {
        "type" : "text"
      },
      "mobile" : {
        "type" : "text",
        "index": false
      }
    }
  }
}
```

### Index Options

- 四种不同级别的 Index Options 配置，可以控制倒排索引记录的内容
  - docs - 记录 doc id
  - freqs - 记录 doc id 和 term frequencies
  - positions - 记录 doc id / term frequencies / term position
  - offsets - doc id / term frequencies / term posistion / character offects
- Text 类型默认记录 postions，其他默认为 docs
- 记录内容越多，占用存储空间越大

### null_value

- 需要对 NULL 值实现搜索
- 只有 Keyword 类型支持设定 null_Value

```json
DELETE users
PUT users
{
  "mappings" : {
    "properties" : {
      "firstName" : {
        "type" : "text"
      },
      "lastName" : {
        "type" : "text"
      },
      "mobile" : {
        "type" : "keyword",  // 这个如果是text 无法设置为空
        "null_value": "NULL"
      }
    }
  }
}

PUT users/_doc/1
{
  "firstName":"Leo",
  "lastName": "Xin",
  "mobile": null
}

PUT users/_doc/2
{
  "firstName":"Leo2",
  "lastName": "Xin2"

}

GET users/_search
{
  "query": {
    "match": {
      "mobile":"NULL"
    }
  }
}
```

### copy_to

- _all 在 7 中已经被 copy_to 所替代
- 满足一些特定的搜索需求
- copy_to 将字段的数值拷贝到目标字段，实现类似 _all 的作用
- copy_to 的目标字段不出现在 _source 中

```json
DELETE users
PUT users
{
  "mappings": {
    "properties": {
      "firstName":{
        "type": "text",
        "copy_to": "fullName"
      },
      "lastName":{
        "type": "text",
        "copy_to": "fullName"
      }
    }
  }
}

PUT users/_doc/1
{
  "firstName":"Leo",
  "lastName": "Xin"
}

GET users/_search?q=fullName:(Leo Xin)

POST users/_search
{
  "query": {
    "match": {
       "fullName":{
        "query": "Leo Xin",
        "operator": "and"
      }
    }
  }
}
```

### 数组类型

- Elasticsearch 中不提供专门的数组类型。但是任何字段，都可以包含多个相同类型的数值

```json
PUT users/_doc/1
{
  "name":"onebird",
  "interests":"reading"
}

PUT users/_doc/1
{
  "name":"twobirds",
  "interests":["reading","music"]
}

POST users/_search
{
  "query": {
    "match_all": {}
  }
}

GET users/_mapping
```

> PS: text 类型会使用默认分词器分词，当然你也可以为他指定特定的分词器。如果定义成 keyword 类型，那么默认就不会对其进行分词。
> ES 对字符串类型的 mapping 设定，会将其定义成 text，同时为他定义一个叫做 keyword 的子字段。keyword 只是他的名字，你也可以定义成 kw。这个字段的类型是 keyword（这是一个类型的关键字）
> 多字段类型情况下，你可以查询 title，也可以查询 title.keyword 查询类型为 keyword 的子字段

## 多字段特性及 Mapping 中配置自定义 Analyzer

### 多字段类型

- 多字段特性
  - 厂家名字实现精确匹配
    - 增加一个 keyword 字段
  - 使用不同的 analyzer
    - 不同语言
    - pinyin 字段的搜索
    - 还支持为搜索和索引指定不同的 analyzer

![29][29]

### Excat values v.s Full Text

- Excat Values ：包括数字 / 日期 / 具体一个字符串 （例如 “Apple Store”）
  - Elasticsearch 中的 keyword
- 全文本，非结构化的文本数据
  - Elasticsearch 中的 text

![30][30]

### Excat values 不需要被分词

- Elaticsearch 为每一个字段创建一个倒排索引
  - Exact Value 在索引时，不需要做特殊的分词处理

![31][31]

### 自定义分词

- 当 Elasticsearch 自带的分词器无法满足时，可以自定义分词器。通过自组合不同的组件实现
  - Character Filter
  - Tokenizer
  - Token Filter

### Character Filters

- 在 Tokenizer 之前对文本进行处理，例如增加删除及替换字符。可以配置多个 Character Filters。会影响 Tokenizer 的 position 和 offset 信息
- 一些自带的 Character Filters
  - HTML strip - 去除 html 标签
  - Mapping - 字符串替换
  - Pattern replace - 正则匹配替换

### Tokenizer

- 将原始的文本按照一定的规则，切分为词（term or token）
- Elasticsearch 内置的 Tokenizers
  - whitespace | standard | uax_url_email | pattern | keyword | path hierarchy
- 可以用 JAVA 开发插件，实现自己的 Tokenizer

### Token Filters

- 将 Tokenizer 输出的单词，进行增加、修改、删除
- 自带的 Token Filters
  - Lowercase | stop | synonym（添加近义词）

### Demo char_filter

- char_filter

```json
POST _analyze
{
  "tokenizer":"keyword",
  "char_filter":["html_strip"],
  "text": "<b>hello world</b>"
}
//结果
{
  "tokens" : [
    {
      "token" : "hello world",
      "start_offset" : 3,
      "end_offset" : 18,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

- 使用 char filter 进行替换

```json
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "mapping",
        "mappings" : [ "- => _"]
      }
    ],
  "text": "123-456, I-test! test-990 650-555-1234"
}
```

- char filter 替换表情符号

```json
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "mapping",
        "mappings" : [ ":) => happy", ":( => sad"]
      }
    ],
    "text": ["I am felling :)", "Feeling :( today"]
}
```

- 正则表达式

```json
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "pattern_replace",
        "pattern" : "http://(.*)",
        "replacement" : "$1"
      }
    ],
    "text" : "http://www.elastic.co"
}
```

### Demo tokenizer

- 通过路劲切分

```json
POST _analyze
{
  "tokenizer":"path_hierarchy",
  "text":"/user/ymruan/a"
}

// 结果
{
  "tokens" : [
    {
      "token" : "/user",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/user/ymruan",
      "start_offset" : 0,
      "end_offset" : 12,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/user/ymruan/a",
      "start_offset" : 0,
      "end_offset" : 14,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

- token_filters

```json
GET _analyze
{
  "tokenizer": "whitespace", 
  "filter": ["stop","snowball"], //on the a
  "text": ["The gilrs in China are playing this game!"]
}
// 结果
{
  "tokens" : [
    {
      "token" : "The", //大写的The 不做过滤
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "gilr",
      "start_offset" : 4,
      "end_offset" : 9,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "China",
      "start_offset" : 13,
      "end_offset" : 18,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "play",
      "start_offset" : 23,
      "end_offset" : 30,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "game!",
      "start_offset" : 36,
      "end_offset" : 41,
      "type" : "word",
      "position" : 7
    }
  ]
}
```

- 加入 lowercase 后，The 被当成 stopword 删除

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase","stop","snowball"],
  "text": ["The gilrs in China are playing this game!"]
}
```

### 自定义 analyzer

- 官网自定义分词器的标准格式

```json
PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": { ... custom character filters ... },//字符过滤器
      "tokenizer": { ... custom tokenizers ... },//分词器
      "filter": { ... custom token filters ... }, //词单元过滤器
      "analyzer": { ... custom analyzers ... }
    }
  }
}
```

- 自定义分词器

```json
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer":{
          "type":"custom",
          "char_filter":[
            "emoticons"
          ],
          "tokenizer":"punctuation",
          "filter":[
            "lowercase",
            "english_stop"
          ]
        }
      },
      "tokenizer": {
        "punctuation":{
          "type":"pattern",
          "pattern": "[ .,!?]"
        }
      },
      "char_filter": {
        "emoticons":{
          "type":"mapping",
          "mappings" : [ 
            ":) => happy",
            ":( => sad"
          ]
        }
      },
      "filter": {
        "english_stop":{
          "type":"stop",
          "stopwords":"_english_"
        }
      }
    }
  }
}
```

## Dynamic Template和Index Template

### 管理很多索引

- 集群上的索引会越来越多，例如，你会为你的日志每天创建一个索引
  - 使用多个索引可以让你更好的管理你的数据，提高性能
  - logs-2019-05-01
  - logs-2019-05-02
  - logs-2019-05-03

### 什么是 Index Template

- Index Templates - 帮助你设定 Mappings 和 Settings，并按照一定的规则，自动匹配到新创建的索引之上
  - 模版仅在一个索引被新创建时，才会产生作用。修改模版不会影响已创建的索引
  - 你可以设定多个索引模版，这些设置会被"merge"在一起
  - 你可以指定"order"的数值，控制"merging"的过程

### Index Template 的工作方式

- 当一个索引被新创建时
  - 应用 Elasticsearch 默认的 settings 和 mappings
  - 应用 order 数值低的 Index Template 中的设定
  - 应用 order 高的 Index Template 中的设定，之前的设定会被覆盖
  - 应用创建索引时，用户所指定的 Settings 和 Mappings，并覆盖之前模版中的设定

```json
// 数字字符串被映射成text，日期字符串被映射成日期
PUT ttemplate/_doc/1
{
  "someNumber": "1",
  "someDate":"2019/01/01"
}
GET ttemplate/_mapping

// 创建默认模板
PUT _template/template_default
{
  "index_patterns": ["*"],
  "order" : 0,
  "version": 1,
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas":1
  }
}

// 创建第二个模板
PUT /_template/template_test
{
  "index_patterns" : ["test*"],
  "order" : 1,
  "settings" : {
    "number_of_shards": 1,
    "number_of_replicas" : 2
  },
  "mappings" : {
    "date_detection": false,
    "numeric_detection": true
  }
}

// 查看template信息
GET /_template/template_default
GET /_template/temp*

// 写入新的数据，index以test开头
PUT testtemplate/_doc/1
{
  "someNumber":"1",
  "someDate":"2019/01/01"
}

// 查看数据类型推断和replica
GET testtemplate/_mapping
GET testtemplate/_settings

PUT testmy
{
  "settings":{
    "number_of_replicas":5
  }
}

PUT testmy/_doc/1
{
  "key":"value"
}

GET testmy/_settings
DELETE testmy
DELETE /_template/template_default
DELETE /_template/template_test
```

### 什么是 Dynamic Template

- 根据 Elasticsearch 识别的数据类型，结合字段名称，来动态设定字段类型
  - 所有的字符串类型都设定成 Keyword，或者关闭 keyword 字段
  - is 开头的字段都设置成 boolean
  - long_开头的都设置成 long 类型

### Dynamic Tempate

- Dynamic Tempate 是定义在在某个索引的 Mapping 中
- Template有一个名称
- 匹配规则是一个数组
- 为匹配到字段设置 Mapping

![32][32]

### 匹配规则参数

- match_mapping_type∶ 匹配自动识别的字段类型，如 string，boolean等
- match， unmatch∶ 匹配字段名
- path_match, path_unmatch

![33][33]

```json

// Dynaminc Mapping 根据类型和字段名
DELETE my_index

PUT my_index/_doc/1
{
  "firstName":"Ruan",
  "isVIP":"true"
}

GET my_index/_mapping
DELETE my_index
PUT my_index
{
  "mappings": {
    "dynamic_templates": [
    {
        "strings_as_boolean": {
          "match_mapping_type": "string",
          "match":"is*",
          "mapping": {
            "type": "boolean"
          }
        }
      },
      {
        "strings_as_keywords": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "keyword"
          }
        }
      }
    ]
  }
}


DELETE my_index
// 结合路径
PUT my_index
{
  "mappings": {
    "dynamic_templates": [
      {
        "full_name": {
          "path_match":   "name.*",
          "path_unmatch": "*.middle",
          "mapping": {
            "type":       "text",
            "copy_to":    "full_name"
          }
        }
      }
    ]
  }
}

PUT my_index/_doc/1
{
  "name": {
    "first":  "John",
    "middle": "Winston",
    "last":   "Lennon"
  }
}

GET my_index/_search?q=full_name:John
```

## 聚合（Aggregation）

![34][34]

- Elasticsearch 除搜索以外，提供的针对 ES 数据进行统计分析的功能
  - 实时性
  - Hadoop (T+1)
- 通过聚合，我们会得到一个数据的概念，是分析和总结全套的数据，而不是寻找单个文档
  - 尖沙咀和香港岛的客房数量
  - 不同的价格区间，可预定的经济型酒店和五星级酒店的数量
- 高性能，只需要一条语句，就可以从 ES 得到分析结果
  - 无需再客户端自己去实现分析逻辑

### Kibana 可视化报表 - 聚合分析

- 公司程序员的工作岗位分布
- 公司采用的编程框架分布
- 公司员工薪水分布客户的地理位置分布
- 订单的增长情况
- 等等

### 集合的分类

- Bucket Aggregation - 一些列满足特定条件的文档的集合
- Metric Aggregation - 一些数学运算，可以对文档字段进行统计分析
- Pipeline Aggregation - 对其他的聚合结果进行二次聚合
- Matrix Aggregation - 支持对多个字段的操作并提供一个结果矩阵

### Bucket & Metric

- Metric - 一些系统的统计方法（类似 count）
- Bucket - 一组满足条件的文档（group by）

![35][35]

#### Bucket

- 一些例子
  - 杭州属于浙江 / 演员是男或女
  - 嵌套关系 - 杭州属于浙江属于中国属于亚洲
- ES 提供了许多的类型的 Bucket，帮助用多种方式划分文档
  - Tern & Range (时间 / 年龄区间 / 地理位置)

![36][36]

#### Metric

- Metric 会基于数据集计算结果，除了支持在字段上进行计算，同样也支持在脚本（painless script）产生的结果之上进行计算
- 大多数 Metric 是数学计算，仅输出一个值
  - min / max / sum / avg /cardinality
- 部分 metric 支持输出多个数值
  - stats / percentiles / percentile_ranks

### 一个 Bucket 例子

![37][37]

![38][38]

![39][39]

```json
// 需要通过Kibana导入Sample Data的飞机航班数据。
// 按照目的地进行分桶统计
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			}
		}
	}
}

// 查看航班目的地的统计信息，增加平均，最高最低价格
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			},
			"aggs":{
				"avg_price":{
					"avg":{
						"field":"AvgTicketPrice"
					}
				},
				"max_price":{
					"max":{
						"field":"AvgTicketPrice"
					}
				},
				"min_price":{
					"min":{
						"field":"AvgTicketPrice"
					}
				}
			}
		}
	}
}

// 价格统计信息+天气信息
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			},
			"aggs":{
					"stats":{
						"field":"AvgTicketPrice"
					}
				},
				"wather":{
				  "terms": {
				    "field": "DestWeather",
				    "size": 5
				  }
				}

			}
		}
	}
}
```

## 参考

- [为什么不再支持单个Index下，多个Tyeps](https://www.elastic.co/cn/blog/moving-from-types-to-typeless-apis-in-elasticsearch-7-0)
- [CAT Index API](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/cat-indices.html)
- [Cluster API](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/cluster.html)
- [CAT Shards API](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/cat-shards.html)
- [Document API](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/docs.html)
- [Mapping Parameters](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/mapping-params.html)
- [Index Templates](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/indices-templates.html)
- [Dynamic Template](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/dynamic-mapping.html)
- 《Elasticsearch核心技术与实战》
- https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/inverted-index.html
- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/indices-analyze.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html
- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/search-search.html
- https://searchenginewatch.com/sew/news/2065080/search-engines-101
- https://www.huffpost.com/entry/search-engines-101-part-i_b_1104525
- https://www.entrepreneur.com/article/176398
- https://www.searchtechnologies.com/meaning-of-relevancy
- https://baike.baidu.com/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%8F%91%E5%B1%95%E5%8F%B2/2422574
- https://www.elastic.co/guide/en/elasticsearch/reference/7.0/search-uri-request.html
- https://www.elastic.co/guide/en/elasticsearch/reference/7.0/search-search.html
- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/search-aggregations.html

[1]: /images/big-data/es-03/1.jpg
[2]: /images/big-data/es-03/2.jpg
[3]: /images/big-data/es-03/3.jpg
[4]: /images/big-data/es-03/4.jpg
[5]: /images/big-data/es-03/5.jpg
[6]: /images/big-data/es-03/6.jpg
[7]: /images/big-data/es-03/7.jpg
[8]: /images/big-data/es-03/8.jpg
[9]: /images/big-data/es-03/9.jpg
[10]: /images/big-data/es-03/10.jpg
[11]: /images/big-data/es-03/11.jpg
[12]: /images/big-data/es-03/12.jpg
[13]: /images/big-data/es-03/13.jpg
[14]: /images/big-data/es-03/14.jpg
[15]: /images/big-data/es-03/15.jpg
[16]: /images/big-data/es-03/16.jpg
[17]: /images/big-data/es-03/17.jpg
[18]: /images/big-data/es-03/18.jpg
[19]: /images/big-data/es-03/19.jpg
[20]: /images/big-data/es-03/20.jpg
[21]: /images/big-data/es-03/21.jpg
[22]: /images/big-data/es-03/22.jpg
[23]: /images/big-data/es-03/23.jpg
[24]: /images/big-data/es-03/24.jpg
[25]: /images/big-data/es-03/25.jpg
[26]: /images/big-data/es-03/26.jpg
[27]: /images/big-data/es-03/27.jpg
[28]: /images/big-data/es-03/28.jpg
[29]: /images/big-data/es-03/29.jpg
[30]: /images/big-data/es-03/30.jpg
[31]: /images/big-data/es-03/31.jpg
[32]: /images/big-data/es-03/32.jpg
[33]: /images/big-data/es-03/33.jpg
[34]: /images/big-data/es-03/34.jpg
[35]: /images/big-data/es-03/35.jpg
[36]: /images/big-data/es-03/36.jpg
[37]: /images/big-data/es-03/37.jpg
[38]: /images/big-data/es-03/38.jpg
[39]: /images/big-data/es-03/39.jpg

<style>
  img {
    zoom: 50%;
  }
</style>
