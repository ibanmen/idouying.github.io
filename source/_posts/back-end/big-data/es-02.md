---
title: 重学 Elastic Stack 之 Elasticsearch 核心概念
date: 2021-01-31 13:00:00
categories: BigData
tags:
  - Elastic Stack
  - ES
---

Elasticsearch 是使用 Java 编写的一种开源搜索引擎，它在内部使用 Luence 做索引与搜索，通过对 Lucene 的封装，提供了一套简单一致的 RESTful API。

Elasticsearch 也是一种分布式的搜索引擎架构，可以很简单地扩展到上百个服务节点，并支持 PB 级别的数据查询，使系统具备高可用和高并发性。

本文介绍 Elasticsearch 核心概念和索引写入过程。

<!--more-->

## ES 核心概念

Elasticsearch 的核心概念如下：

- **Cluster**：集群，由一个或多个 Elasticsearch 节点组成。

- **Node**：节点，组成 Elasticsearch 集群的服务单元，同一个集群内节点的名字不能重复。通常在一个节点上分配一个或者多个分片。

- **Shards**：分片，当索引上的数据量太大的时候，我们通常会将一个索引上的数据进行水平拆分，拆分出来的每个数据库叫作一个分片。

  - 在一个多分片的索引中写入数据时，通过路由来确定具体写入那一个分片中，所以在创建索引时需要指定分片的数量，并且分片的数量一旦确定就不能更改。

  - 分片后的索引带来了规模上（数据水平切分）和性能上（并行执行）的提升。每个分片都是 Luence 中的一个索引文件，每个分片必须有一个主分片和零到多个副本分片。

- **Replicas**：备份也叫作副本，是指对主分片的备份。主分片和备份分片都可以对外提供查询服务，写操作时先在主分片上完成，然后分发到备份上。

  - 当主分片不可用时，会在备份的分片中选举出一个作为主分片，所以备份不仅可以提升系统的高可用性能，还可以提升搜索时的并发性能。但是若副本太多的话，在写操作时会增加数据同步的负担。

- **Index**：索引，由一个和多个分片组成，通过索引的名字在集群内进行唯一标识。

- **Type**：类别，指索引内部的逻辑分区，通过 Type 的名字在索引内进行唯一标识。在查询时如果没有该值，则表示在整个索引中查询。

- **Document**：文档，索引中的每一条数据叫作一个文档，类似于关系型数据库中的一条数据通过 _id 在 Type 内进行唯一标识。

- **Settings**：对集群中索引的定义，比如一个索引默认的分片数、副本数等信息。

- **Mapping**：类似于关系型数据库中的表结构信息，用于定义索引中字段（Field）的存储类型、分词方式、是否存储等信息。Elasticsearch 中的 Mapping 是可以动态识别的。

  - 如果没有特殊需求，则不需要手动创建 Mapping，因为 Elasticsearch 会自动根据数据格式识别它的类型，但是当需要对某些字段添加特殊属性（比如：定义使用其他分词器、是否分词、是否存储等）时，就需要手动设置 Mapping 了。一个索引的 Mapping 一旦创建，若已经存储了数据，就不可修改了。

- **Analyzer**：字段的分词方式的定义。一个 Analyzer 通常由一个 Tokenizer、零到多个 Filter 组成。

  - 比如默认的标准 Analyzer 包含一个标准的 Tokenizer 和三个 Filter：Standard Token Filter、Lower Case Token Filter、Stop Token Filter。

## Elasticsearch 的节点的分类如下

**主节点（Master Node）**：也叫作主节点，主节点负责创建索引、删除索引、分配分片、追踪集群中的节点状态等工作。Elasticsearch 中的主节点的工作量相对较轻。

用户的请求可以发往任何一个节点，并由该节点负责分发请求、收集结果等操作，而并不需要经过主节点转发。

通过在配置文件中设置 node.master=true 来设置该节点成为候选主节点（但该节点不一定是主节点，主节点是集群在候选节点中选举出来的），在 Elasticsearch 集群中只有候选节点才有选举权和被选举权。其他节点是不参与选举工作的。

**数据节点（Data Node）**：数据节点，负责数据的存储和相关具体操作，比如索引数据的创建、修改、删除、搜索、聚合。

所以，数据节点对机器配置要求比较高，首先需要有足够的磁盘空间来存储数据，其次数据操作对系统 CPU、Memory 和 I/O 的性能消耗都很大。

通常随着集群的扩大，需要增加更多的数据节点来提高可用性。通过在配置文件中设置 node.data=true 来设置该节点成为数据节点。

**客户端节点（Client Node）**：就是既不做候选主节点也不做数据节点的节点，只负责请求的分发、汇总等，也就是下面要说到的协调节点的角色。

其实任何一个节点都可以完成这样的工作，单独增加这样的节点更多地是为了提高并发性。
可在配置文件中设置该节点成为数据节点：

```bash
node.master=falsenode.data=false
```

**部落节点（Tribe Node）**：部落节点可以跨越多个集群，它可以接收每个集群的状态，然后合并成一个全局集群的状态。

它可以读写所有集群节点上的数据，在配置文件中通过如下设置使节点成为部落节点：

```bash
tribe:  one:    cluster.name: cluster_one  two:    cluster.name: cluster_two
```

> 因为 Tribe Node 要在 Elasticsearch 7.0 以后移除，所以不建议使用。

**协调节点（Coordinating Node）**：协调节点，是一种角色，而不是真实的 Elasticsearch 的节点，我们没有办法通过配置项来配置哪个节点为协调节点。集群中的任何节点都可以充当协调节点的角色。

当一个节点 A 收到用户的查询请求后，会把查询语句分发到其他的节点，然后合并各个节点返回的查询结果，最好返回一个完整的数据集给用户。

在这个过程中，节点 A 扮演的就是协调节点的角色。由此可见，协调节点会对 CPU、Memory 和 I/O 要求比较高。

集群的状态有 Green、Yellow 和 Red 三种，如下所述：

- **Green**：绿色，健康。所有的主分片和副本分片都可正常工作，集群 100% 健康。

- **Yellow**：黄色，预警。所有的主分片都可以正常工作，但至少有一个副本分片是不能正常工作的。此时集群可以正常工作，但是集群的高可用性在某种程度上被弱化。

- **Red**：红色，集群不可正常使用。集群中至少有一个分片的主分片及它的全部副本分片都不可正常工作。

  - 这时虽然集群的查询操作还可以进行，但是也只能返回部分数据（其他正常分片的数据可以返回），而分配到这个分片上的写入请求将会报错，最终会导致数据的丢失。


## 3C 和脑裂

**共识性（Consensus）**

共识性是分布式系统中最基础也最主要的一个组件，在分布式系统中的所有节点必须对给定的数据或者节点的状态达成共识。

虽然现在有很成熟的共识算法如 Raft、Paxos 等，也有比较成熟的开源软件如 Zookeeper。

但是 Elasticsearch 并没有使用它们，而是自己实现共识系统 zen discovery。

**Elasticsearch 之父 Shay Banon 解释了其中主要的原因：**“zen discovery 是 Elasticsearch 的一个核心的基础组件，zen discovery 不仅能够实现共识系统的选择工作，还能够很方便地监控集群的读写状态是否健康。当然，我们也不保证其后期会使用 Zookeeper 代替现在的 zen discovery”。

zen discovery 模块以 “八卦传播”（Gossip）的形式实现了单播（Unicat）：单播不同于多播（Multicast）和广播（Broadcast）。节点间的通信方式是一对一的。

**并发（Concurrency）**

Elasticsearch 是一个分布式系统。写请求在发送到主分片时，同时会以并行的形式发送到备份分片，但是这些请求的送达时间可能是无序的。

在这种情况下，Elasticsearch 用乐观并发控制（Optimistic Concurrency Control）来保证新版本的数据不会被旧版本的数据覆盖。

乐观并发控制是一种乐观锁，另一种常用的乐观锁即多版本并发控制（Multi-Version Concurrency Control）。

它们的主要区别如下：

- **乐观并发控制（OCC）**：是一种用来解决写 - 写冲突的无锁并发控制，认为事务间的竞争不激烈时，就先进行修改，在提交事务前检查数据有没有变化，如果没有就提交，如果有就放弃并重试。乐观并发控制类似于自选锁，适用于低数据竞争且写冲突比较少的环境。

- **多版本并发控制（MVCC）**：是一种用来解决读 - 写冲突的无所并发控制，也就是为事务分配单向增长的时间戳，为每一个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。

  - 这样在读操作不用阻塞操作且写操作不用阻塞读操作的同时，避免了脏读和不可重复读。

**一致性（Consistency）**

Elasticsearch 集群保证写一致性的方式是在写入前先检查有多少个分片可供写入，如果达到写入条件，则进行写操作，否则，Elasticsearch 会等待更多的分片出现，默认为一分钟。

有如下三种设置来判断是否允许写操作：

- **One**：只要主分片可用，就可以进行写操作。

- **All**：只有当主分片和所有副本都可用时，才允许写操作。

- **Quorum（k-wu-wo/reng，法定人数）**：是 Elasticsearch 的默认选项。当有大部分的分片可用时才允许写操作。其中，对 “大部分” 的计算公式为 int ((primary+number_of_replicas)/2)+1。

Elasticsearch 集群保证读写一致性的方式是，为了保证搜索请求的返回结果是最新版本的文档，备份可以被设置为 Sync（默认值），写操作在主分片和备份分片同时完成后才会返回写请求的结果。

这样，无论搜索请求至哪个分片都会返回最新的文档。但是如果我们的应用对写要求很高，就可以通过设置 replication=async 来提升写的效率，如果设置 replication=async，则只要主分片的写完成，就会返回写成功。

**脑裂**

在 Elasticsearch 集群中主节点通过 Ping 命令来检查集群中的其他节点是否处于可用状态，同时非主节点也会通过 Ping 来检查主节点是否处于可用状态。

当集群网络不稳定时，有可能会发生一个节点 Ping 不通 Master 节点，则会认为 Master 节点发生了故障，然后重新选出一个 Master 节点，这就会导致在一个集群内出现多个 Master 节点。

当在一个集群中有多个 Master 节点时，就有可能会导致数据丢失。我们称这种现象为脑裂。

## 事务日志

Lucene 为了加快写索引的速度，采用了延迟写入的策略。

虽然这种策略提高了写入的效率，但其最大的弊端是，如果数据在内存中还没有持久化到磁盘上时发生了类似断电等不可控情况，就可能丢失数据。

为了避免丢失数据，Elasticsearch 添加了事务日志（Translog），事务日志记录了所有还没有被持久化磁盘的数据。

Elasticsearch 写索引的具体过程如下：首先，当有数据写入时，为了提升写入的速度，并没有数据直接写在磁盘上，而是先写入到内存中，但是为了防止数据的丢失，会追加一份数据到事务日志里。

因为内存中的数据还会继续写入，所以内存中的数据并不是以段的形式存储的，是检索不到的。

总之，Elasticsearch 是一个准实时的搜索引擎，而不是一个实时的搜索引擎。

此时的状态如图所示：

![1][1]

然后，当达到默认的时间（1 秒钟）或者内存的数据达到一定量时，会触发一次刷新（Refresh）。

刷新的主要步骤如下：

- 将内存中的数据刷新到一个新的段中，但是该段并没有持久化到硬盘中，而是缓存在操作系统的文件缓存系统中。虽然数据还在内存中，但是内存里的数据和文件缓存系统里的数据有以下区别。

  - 内存使用的是 JVM 的内存，而文件缓存系统使用的是操作系统的内存；内存的数据不是以段的形式存储的，并且可以继续向内存里写数据。文件缓存系统中的数据是以段的形式存储的，所以只能读，不能写；内存中的数据是搜索不到，文件缓存系统中的数据是可以搜索的。

- 打开保存在文件缓存系统中的段，使其可被搜索。

- 清空内存，准备接收新的数据。日志不做清空处理。

此时的状态如图所示：

![2][2]

最后，刷新（Flush）。当日志数据的大小超过 512MB 或者时间超过 30 分钟时，需要触发一次刷新。

刷新的主要步骤如下：

- 在文件缓存系统中创建一个新的段，并把内存中的数据写入，使其可被搜索。
- 清空内存，准备接收新的数据。
- 将文件系统缓存中的数据通过 Fsync 函数刷新到硬盘中。
- 生成提交点。
- 删除旧的日志，创建一个空的日志。

此时的状态如图所示：

![3][3]

由上面索引创建的过程可知，内存里面的数据并没有直接被刷新（Flush）到硬盘中，而是被刷新（Refresh）到了文件缓存系统中，这主要是因为持久化数据十分耗费资源，频繁地调用会使写入的性能急剧下降。

所以 Elasticsearch，为了提高写入的效率，利用了文件缓存系统和内存来加速写入时的性能，并使用日志来防止数据的丢失。

在需要重启时，Elasticsearch 不仅要根据提交点去加载已经持久化过的段，还需要根据 Translog 里的记录，把未持久化的数据重新持久化到磁盘上。

根据上面对 Elasticsearch，写操作流程的介绍，我们可以整理出一个索引数据所要经历的几个阶段，以及每个阶段的数据的存储方式和作用，如图所示：

![4][4]

## 在集群中写索引

假设我们有如图所示（图片来自官网）的一个集群，该集群由三个节点组成（Node 1、Node 2 和 Node 3），包含一个由两个主分片和每个主分片由两个副本分片组成的索引。

![5][5]

其中，标星号的 Node 1 是 Master 节点，负责管理整个集群的状态；p1 和 p2 是主分片；r0 和 r1 是副本分片。为了达到高可用，Master 节点避免将主分片和副本放在同一个节点。

将数据分片是为了提高可处理数据的容量和易于进行水平扩展，为分片做副本是为了提高集群的稳定性和提高并发量。

在主分片挂掉后，会从副本分片中选举出一个升级为主分片，当副本升级为主分片后，由于少了一个副本分片，所以集群状态会从 Green 改变为 Yellow，但是此时集群仍然可用。

在一个集群中有一个分片的主分片和副本分片都挂掉后，集群状态会由 Yellow 改变为 Red，集群状态为 Red 时集群不可正常使用。

由上面的步骤可知，副本分片越多，集群的可用性就越高，但是由于每个分片都相当于一个 Lucene 的索引文件，会占用一定的文件句柄、内存及 CPU，并且分片间的数据同步也会占用一定的网络带宽，所以，索引的分片数和副本数并不是越多越好。

写索引时只能写在主分片上，然后同步到副本上，那么，一个数据应该被写在哪个分片上呢？

如图所示，如何知道一个数据应该被写在 p0 还是 p1 上呢答案就是路由（routing），路由公式如下：

```bash
shard = hash(routing)%number_of_primary_shards
```

其中，Routing 是一个可选择的值，默认是文档的 _id（文档的唯一主键，文档在创建时，如果文档的 _id 已经存在，则进行更新，如果不存在则创建）。

后面会介绍如何通过自定义 Routing 参数使查询落在一个分片中，而不用查询所有的分片，从而提升查询的性能。

Routing 通过 Hash 函数生成一个数字，将这个数字除以 number_of_primary_shards（分片的数量）后得到余数。

这个分布在 0 到 number_of_primary_shards - 1 之间的余数，就是我们所寻求的文档所在分片的位置。

这也就说明了一旦分片数定下来就不能再改变的原因，因为分片数改变之后，所有之前的路由值都会变得无效，前期创建的文档也就找不到了。

由于在 Elasticsearch 集群中每个节点都知道集群中的文档的存放位置（通过路由公式定位），所以每个节点都有处理读写请求的能力。

在一个写请求被发送到集群中的一个节点后，此时，该节点被称为协调点（Coordinating Node），协调点会根据路由公式计算出需要写到哪个分片上，再将请求转发到该分片的主分片节点上。

![6][6]

写操作的流程如下：

- 客户端向 Node 1（协调节点）发送写请求。

- Node 1 通过文档的 _id（默认是 _id，但不表示一定是 _id）确定文档属于哪个分片（在本例中是编号为 0 的分片）。请求会被转发到主分片所在的节点 Node 3 上。

- Node 3 在主分片上执行请求，如果成功，则将请求并行转发到 Node 1 和 Node 2 的副本分片上。

- 一旦所有的副本分片都报告成功（默认），则 Node 3 将向协调节点报告成功，协调节点向客户端报告成功。

## 集群中的查询流程

根据 Routing 字段进行的单个文档的查询，在 Elasticsearch 集群中可以在主分片或者副本分片上进行。

![7][7]

查询字段刚好是 Routing 的分片字段如 “_id” 的查询流程如下：

- 客户端向集群发送查询请求，集群再随机选择一个节点作为协调点（Node 1），负责处理这次查询。

- Node 1 使用文档的 routing id 来计算要查询的文档在哪个分片上（在本例中落在了 0 分片上）分片 0 的副本分片存在所有的三个节点上。

- 在这种情况下，协调节点可以把请求转发到任意节点，本例将请求转发到 Node 2 上。

- Node 2 执行查找，并将查找结果返回给协调节点 Node 1，Node 1 再将文档返回给客户端。

当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点（Node 1）。

协调节点的任务是广播查询请求到所有分片（主分片或者副本分片），并将它们的响应结果整合成全局排序后的结果集合。

由上面步骤 3 所示，默认返回给协调节点并不是所有的数据，而是只有文档的 id 和得分 score，因为我们最后只返回给用户 size 条数据，所以这样做的好处是可以节省很多带宽，特别是 from 很大时。

协调节点对收集回来的数据进行排序后，找到要返回的 size 条数据的 id，再根据 id 查询要返回的数据，比如 title、content 等。

![8][8]

取回数据等流程如下：

- Node 3 进行二次排序来找出要返回的文档 id，并向相关的分片提交多个获得文档详情的请求。

- 每个分片加载文档，并将文档返回给 Node 3。

- 一旦所有的文档都取回了，Node 3 就返回结果给客户端。


协调节点收集各个分片查询出来的数据，再进行二次排序，然后选择需要被取回的文档。

例如，如果我们的查询指定了 {"from": 20, "size": 10}，那么我们需要在每个分片中查询出来得分最高的 20+10 条数据，协调节点在收集到 30×n（n 为分片数）条数据后再进行排序。

排序位置在 0-20 的结果会被丢弃，只有从第 21 个开始的 10 个结果需要被取回。这些文档可能来自多个甚至全部分片。

由上面的搜索策略可以知道，在查询时深翻（Deep Pagination）并不是一种好方法。

因为深翻时，from 会很大，这时的排序过程可能会变得非常沉重，会占用大量的 CPU、内存和带宽。因为这个原因，所以强烈建议慎重使用深翻。

分片可以减少每个片上的数据量，加快查询的速度，但是在查询时，协调节点要在收集数 (from+size)×n 条数据后再做一次全局排序。

若这个数据量很大，则也会占用大量的 CPU、内存、带宽等，并且分片查询的速度取决于最慢的分片查询的速度，所以分片数并不是越多越好。

## 参考

- 《Elasticsearch核心技术与实战》

[1]: /images/big-data/es-02/1.jpg
[2]: /images/big-data/es-02/2.jpg
[3]: /images/big-data/es-02/3.jpg
[4]: /images/big-data/es-02/4.jpg
[5]: /images/big-data/es-02/5.jpg
[6]: /images/big-data/es-02/6.jpg
[7]: /images/big-data/es-02/7.jpg
[8]: /images/big-data/es-02/8.jpg


<style>
  img {
    zoom: 50%;
  }
</style>
